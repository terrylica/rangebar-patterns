# Telemetry Logs - AI Context

**Scope**: NDJSON telemetry archives from brute-force sweep experiments on remote ClickHouse.

**Navigation**: [Root CLAUDE.md](/CLAUDE.md)

---

## Directory Convention

```
logs/gen{NNN}/{descriptor}.jsonl
```

Each generation is a self-contained experiment directory. One hypothesis per generation.

---

## Generation Index

| Gen | Directory | Hypothesis                              | Date       | Configs         | Verdict                               |
| --- | --------- | --------------------------------------- | ---------- | --------------- | ------------------------------------- |
| 300 | `gen300/` | Single-feature filters on champion      | 2026-02-06 | 48 + 36 barrier | Marginal (best Kelly +0.011)          |
| 400 | `gen400/` | Multi-feature combo filters (2F+3F+4F)  | 2026-02-07 | 14,224          | Best Kelly +0.165 (160 signals)       |
| 500 | `gen500/` | Cross-asset 2F sweep (12 assets)        | 2026-02-07 | 12,096          | 443 configs positive on 3+ assets     |
| 510 | `gen510/` | Barrier grid on top 5 Gen400 winners    | 2026-02-07 | 180             | Best Kelly +0.157 (TP=0.25x SL=0.50x) |
| 520 | `gen520/` | Multi-threshold SOLUSDT (@250,750,1000) | 2026-02-07 | 3,024           | Best Kelly +0.180 (@750 threshold)    |

---

## NDJSON Schema (Common Fields)

Every JSONL line contains these fields:

```json
{
  "timestamp": "ISO 8601",
  "generation": 400,
  "config_id": "feature1_dir_pXX__feature2_dir_pXX",
  "environment": {
    "symbol": "SOLUSDT",
    "threshold_dbps": 500,
    "clickhouse_host": "remote",
    "template_file": "sql/gen400_2feature_template.sql",
    "template_sha256": "...",
    "git_commit": "b6d84af",
    "quantile_method": "rolling_1000_signal"
  },
  "timing": {
    "query_start_utc": "ISO 8601",
    "query_end_utc": "ISO 8601",
    "query_duration_s": 2
  },
  "results": {
    "filtered_signals": 550,
    "tp_count": 183,
    "sl_count": 360,
    "time_count": 7,
    "win_rate": 0.332,
    "profit_factor": 1.024,
    "kelly_fraction": -0.005
  },
  "skipped": false,
  "error": false
}
```

---

## File Types

| Extension   | Purpose                                                  |
| ----------- | -------------------------------------------------------- |
| `.jsonl`    | NDJSON telemetry (committed directly if under 1MB)       |
| `.jsonl.br` | Brotli-compressed NDJSON (for files >1MB, LFS-tracked)   |
| `.txt`      | Human-readable report snapshots (generated by report.sh) |
| `_ARCHIVED` | Superseded data — kept for provenance, never deleted     |

### Compression Convention

JSONL files are committed **directly** when under 1MB. Files exceeding 1MB are compressed
with **Brotli --quality=11** (state-of-the-art, 14–21x ratio on NDJSON):

- A pre-commit hook auto-compresses staged `.jsonl` files >1MB to `.jsonl.br`
- Git LFS tracks `.jsonl.br` files (see `.gitattributes`)
- Both `.jsonl` and `.jsonl.br` are tracked in git (no gitignore)
- To decompress: `brotli -d -c logs/gen400/3feature.jsonl.br > logs/gen400/3feature.jsonl`
- Install brotli: `brew install brotli`

---

## Archiving Rules

1. **Never delete** — superseded data gets `_ARCHIVED` suffix
2. **Append-only** — JSONL files only grow, never truncated
3. **One generation per directory** — no mixing experiments
4. **Git-tracked** — all JSONL files committed and pushed for remote backup
5. **Git LFS** — consider for files >50MB (none yet)

---

## Common Queries

```bash
# Top 10 by Kelly across all Gen400 phases
python3 -c "
import json
results = []
for f in ['logs/gen400/2feature.jsonl','logs/gen400/3feature.jsonl','logs/gen400/4feature.jsonl']:
    for line in open(f):
        d = json.loads(line)
        if not d.get('error') and not d.get('skipped'):
            k = d['results'].get('kelly_fraction')
            if k is not None and d['results']['filtered_signals'] >= 100:
                results.append((k, d['config_id'], d['results']['filtered_signals']))
results.sort(reverse=True)
for k, cid, n in results[:10]:
    print(f'Kelly={k:+.5f}  signals={n:>5d}  {cid}')
"

# Count positive Kelly per phase
for f in logs/gen400/*.jsonl; do
  echo -n "$(basename $f): "
  python3 -c "
import json
pos = sum(1 for l in open('$f') if (d:=json.loads(l)) and not d.get('error') and not d.get('skipped') and (k:=d['results'].get('kelly_fraction')) is not None and k > 0 and d['results']['filtered_signals'] >= 100)
print(pos)
"
done
```

---

## Data Provenance

- **Source**: Remote ClickHouse (`rangebar_cache.range_bars`)
- **Collection**: `scp $RANGEBAR_CH_HOST:/tmp/gen{NNN}_*.jsonl logs/gen{NNN}/`
- **Validation**: All lines must pass `json.loads()` before commit
- **Fixes applied**: `nan` → `null`, `\N` → `NULL` (ClickHouse NULL representation)
